Q.1) 
Linear kernel:
Linear Kernel is used when the data is Linearly separable, that is, it can be separated using a single Line. It is most commonly used kernel when the dataset has
large number of features. One can give the example of text classification where each alphabet is a new feature. So most commony in text classification we use linear kernel.

RBF kernel:
Apart from the classic linear kernel which assumes that the different classes are separated by a straight line, a RBF (radial basis function) kernel is used when the 
boundaries are hypothesized to be curve-shaped.RBF kernel uses two main parameters, gamma and C that are related to the decision region (how spread the region is), and
the penalty for misclassifying a data point respectively.

Polynomial kernel:
The polynomial kernel looks not only at the given features of input samples to determine their similarity, but also combinations of these.It represents the similarity of
vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models.

Q.2) R-Squared only works as intended in a simple linear regression model with one explanatory variable. represents the proportion of the variance for a dependent variable 
that's explained by an independent variable or variables in a regression model.
A residual sum of squares (RSS) is a statistical technique used to measure the amount of variance in a data set that is not explained by a regression model.
The residual sum of squares measures the amount of error remaining between the regression function and the data set.So the variance which is not explained by R-squared 
can be explained by RSS. So RSS is good over R-squared.

Q.3)
The Total SS (TSS or SST) tells you how much variation there is in the dependent variable.Total SS = Σ(Yi – mean of Y)2
Sum of squares is a measure of how a data set varies around a central number (like the mean). 
The Explained SS tells you how much of the variation in the dependent variable your model explained.
Explained SS = Σ(Y-Hat – mean of Y)2.
The residual sum of squares tells you how much of the dependent variable’s variation your model did not explain. It is the sum of the squared differences between 
the actual Y and the predicted Y:
Residual Sum of Squares = Σ e2
R square is a term which explain these terms. R2 = MSS/TSS = (TSS − RSS)/TSS

Q.4)
Gini Impurity is the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution 
in the dataset. When training a decision tree, the best split is chosen by maximizing the Gini Gain, which is calculated by subtracting the weighted impurities of 
the branches from the original impurity.
 
Q.5)
Yes, unregularized decision trees prone to overfitting.In decision trees regularization can be done by pruning the tree. If left to its own device the tree can continue 
to fit till each data point is a different leaf in the tree. This obviously will not generalize well so you have to put in different criteria to stop splitting the nodes 
beyond a point. This can be done by specifying how many minimum data points are needed at each node for splitting.

Q.6)
Ensemble methods are techniques that create multiple models and then combine them to produce improved results. Ensemble methods usually produces more accurate solutions than 
a single model would. This has been the case in a number of machine learning competitions, where the winning solutions used ensemble methods. So all the base models are 
used to make one solution. In classification dataset the output is calculated by voting. The class which get maximum votes is the result. In regression the output is
average of all the outputs generated by the base models.

Q.7)
Bagging and Boosting are similar in that they are both ensemble techniques, where a set of weak learners are combined to create a strong learner that obtains better performance 
than a single one. In the case of Bagging, any element has the same probability to appear in a new data set. However, for Boosting the observations are weighted and therefore 
some of them will take part in the new sets more often.training stage is parallel for Bagging (i.e., each model is built independently), Boosting builds the new learner 
in a sequential way.In Boosting algorithms each classifier is trained on data, taking into account the previous classifiers’ success. After each training step, the weights are 
redistributed. Misclassified data increases its weights to emphasise the most difficult cases. In this way, subsequent learners will focus on them during their training.

Q.8)
After creating the classifiers in random forest, for each Xi,yi i.e. input vectors in the original training set, select all Tk(subsets) which does not include (Xi,yi). 
This subset, is a set of boostrap datasets which does not contain a particular record from the original dataset. This set is called out-of-bag examples.The study of error 
estimates for bagged classifiers gives empirical evidence to show that the out-of-bag estimate is as accurate as using a test set of the same size as 
the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set.

Q.9)
 K-fold Cross Validation(CV) divides the data into folds and ensuring that each fold is used as a testing set at some point.K-Fold CV is where a given data set is split 
into a K number of sections/folds where each fold is used as a testing set at some point. Lets take the scenario of 5-Fold cross validation(K=5). Here, the data set is 
split into 5 folds. In the first iteration, the first fold is used to test the model and the rest are used to train the model.In the second iteration, 2nd fold is used as 
the testing set while the rest serve as the training set. This process is repeated until each fold of the 5 folds have been used as the testing set.

Q.10)
Parameters which define the model architecture are referred to as hyperparameters and thus this process of searching for the ideal model architecture is referred to as 
hyperparameter tuning. hyperparameters are not model parameters and they cannot be directly trained from the data. Model parameters are learned during training when we 
optimize a loss function using something like gradient descent.Whereas the model parameters specify how to transform the input data into the desired output, the hyperparameters 
define how our model is actually structured.

Q.11)
The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated.a value too large may 
result in learning a sub-optimal set of weights too fast or an unstable training process.Larger learning rates result in rapid changes and require fewer training epochs.
A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution.

Q.12)
The goal of any supervised machine learning algorithm is to achieve low bias and low variance. In turn the algorithm should achieve good prediction performance.
Linear machine learning algorithms often have a high bias but a low variance.Nonlinear machine learning algorithms often have a low bias but a high variance.The 
parameterization of machine learning algorithms is often a battle to balance out bias and variance.Increasing the bias will decrease the variance.
Increasing the variance will decrease the bias.There is a trade-off at play between these two concerns and the algorithms you choose and the way you choose to configure 
them are finding different balances in this trade-off for your problem.

Q.13)
Regularization is a technique used for tuning the function by adding an additional penalty term in the error function.The additional term controls the excessively fluctuating 
function such that the coefficients don't take extreme values. This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero.
In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.

Q.14)
Adaboost is more about 'voting weights' and gradient boosting is more about 'adding gradient optimization'.  Adaboost doesn't overfit because it is more about 'organizing people 
to vote' than 'voting'. In fact, if you have a gradient boosting model, you can use it in adaboost along with other models.Adaboost is an Boosting algorithim which increases the 
accuracy by giving more weightage to the target which is misclassified by the model. For the Next sample repeat the same. Finally weak learners combine together makes the strong model.
Gradient Boosting Algorithim increases the accuracy by minimizing the loss function(error which is difference of actual and predicted value) and having them as target for next decision tree building.

Q.15)
While logistic regression makes core assumptions about the observations such as IID (each observation is independent of the others and they all have an identical probability distribution), 
the use of a linear decision boundary is not one of them.